name: Generate Training Dataset

on:
  schedule:
    # Run every Sunday at midnight UTC
    - cron: '0 0 * * 0'

  workflow_dispatch:
    inputs:
      n_samples:
        description: 'Number of samples to generate'
        default: '200'
        type: string

jobs:
  generate:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v3

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Generate dataset
        env:
          N_SAMPLES: ${{ github.event.inputs.n_samples || '200' }}
        run: |
          echo "Generating dataset with $N_SAMPLES samples..."
          python scripts/generate_rose_dataset.py

      - name: Validate dataset
        run: |
          echo "Validating generated datasets..."
          python -c "
          import pandas as pd
          from pathlib import Path

          data_dir = Path('data/processed')

          for csv_file in data_dir.glob('*.csv'):
              df = pd.read_csv(csv_file)
              print(f'{csv_file.name}:')
              print(f'  Rows: {len(df)}')
              print(f'  Columns: {list(df.columns)}')
              print(f'  Nulls: {df.isnull().sum().sum()}')
              assert df.isnull().sum().sum() == 0, f'Found null values in {csv_file.name}'
              print(f'  âœ“ Valid')
              print()
          "

      - name: Upload dataset as artifact
        uses: actions/upload-artifact@v3
        with:
          name: rose-training-dataset
          path: data/processed/*.csv
          retention-days: 30

      - name: Commit dataset
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"

          git add data/processed/*.csv
          git diff --staged --quiet || git commit -m "Dataset: Auto-generate $(date +%Y-%m-%d)"
          git push || echo "Nothing to push"

      # Optional: Upload to Kaggle
      # Uncomment and configure secrets if using Kaggle
      # - name: Upload to Kaggle
      #   env:
      #     KAGGLE_USERNAME: ${{ secrets.KAGGLE_USERNAME }}
      #     KAGGLE_KEY: ${{ secrets.KAGGLE_KEY }}
      #   run: |
      #     pip install kaggle
      #     kaggle datasets version -p ./data/processed -m "Weekly update"
